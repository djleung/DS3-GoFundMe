{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import sleep\n",
    "import requests\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Url</th>\n",
       "      <th>Category</th>\n",
       "      <th>Position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.gofundme.com/f/justiceforjacobblake</td>\n",
       "      <td>Medical</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>https://www.gofundme.com/f/official-navajo-nat...</td>\n",
       "      <td>Medical</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>https://www.gofundme.com/f/help-a-front-line-n...</td>\n",
       "      <td>Medical</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>https://www.gofundme.com/f/Tommy-Rivers-Rest-Up</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>https://www.gofundme.com/f/brandon039s-medical...</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>881</th>\n",
       "      <td>881</td>\n",
       "      <td>https://www.gofundme.com/f/Juneteenth-women-of...</td>\n",
       "      <td>Wishes</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>882</th>\n",
       "      <td>882</td>\n",
       "      <td>https://www.gofundme.com/f/allterrain-wheelcha...</td>\n",
       "      <td>Wishes</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>883</td>\n",
       "      <td>https://www.gofundme.com/f/Marthasdaughter</td>\n",
       "      <td>Wishes</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>884</td>\n",
       "      <td>https://www.gofundme.com/f/x2s8dc-financial-di...</td>\n",
       "      <td>Wishes</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>885</td>\n",
       "      <td>https://www.gofundme.com/f/yahchouche-my-root</td>\n",
       "      <td>Wishes</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>886 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                                Url Category  \\\n",
       "0             0    https://www.gofundme.com/f/justiceforjacobblake  Medical   \n",
       "1             1  https://www.gofundme.com/f/official-navajo-nat...  Medical   \n",
       "2             2  https://www.gofundme.com/f/help-a-front-line-n...  Medical   \n",
       "3             3    https://www.gofundme.com/f/Tommy-Rivers-Rest-Up  Medical   \n",
       "4             4  https://www.gofundme.com/f/brandon039s-medical...  Medical   \n",
       "..          ...                                                ...      ...   \n",
       "881         881  https://www.gofundme.com/f/Juneteenth-women-of...   Wishes   \n",
       "882         882  https://www.gofundme.com/f/allterrain-wheelcha...   Wishes   \n",
       "883         883         https://www.gofundme.com/f/Marthasdaughter   Wishes   \n",
       "884         884  https://www.gofundme.com/f/x2s8dc-financial-di...   Wishes   \n",
       "885         885      https://www.gofundme.com/f/yahchouche-my-root   Wishes   \n",
       "\n",
       "     Position  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           1  \n",
       "4           1  \n",
       "..        ...  \n",
       "881        22  \n",
       "882        22  \n",
       "883        23  \n",
       "884        23  \n",
       "885        23  \n",
       "\n",
       "[886 rows x 4 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydf = pd.read_csv('GFM_url_list.csv', sep = '\\t')\n",
    "mydf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = [\"Url\", \"Category\",\"Position\", \"Title\", \"Location\",\"Amount_Raised\", \"Goal\", \"Number_of_Donators\", \"Length_of_Fundraising\", \"FB_Shares\", \"GFM_hearts\", \"Text\"]\n",
    "mydf = mydf.reindex(columns = headers)\n",
    "\n",
    "full_df = pd.DataFrame(columns = headers)\n",
    "#need to scrape a single url now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_url(row_index):\n",
    "    single_row = mydf.iloc[row_index]\n",
    "    url = single_row[\"Url\"]\n",
    "    category = single_row[\"Category\"]\n",
    "    position = single_row[\"Position\"]\n",
    "    \n",
    "    page = requests.get(url)\n",
    "         \n",
    "    soup = BeautifulSoup(page.text, 'lxml')\n",
    "\n",
    "    try:\n",
    "        content_dic = json.loads(re.sub(';$','',soup.find('script').text.split('= ')[1]))\n",
    "\n",
    "        amount_raised = content_dic['feed']['campaign']['current_amount']\n",
    "        \n",
    "        goal = content_dic['feed']['campaign']['goal_amount']\n",
    "        \n",
    "        NumDonators = content_dic['feed']['campaign']['donation_count']\n",
    "        \n",
    "        timeFundraised = (pd.Timestamp.now() - (pd.to_datetime(content_dic['feed']['campaign']['created_at'])).astimezone(tz=None))\n",
    "    except:\n",
    "        amount_raised = np.nan\n",
    "        goal = np.nan\n",
    "        NumDonators = np.nan\n",
    "        timeFundraised = np.nan\n",
    "        \n",
    "    \n",
    "    title_container = soup.find_all(\"h1\",{\"class\":\"a-campaign-title\"})#<h1 class=\"campaign-title\">Help Rick Muchow Beat Cancer</h1>\n",
    "    \n",
    "    try:\n",
    "        title = title_container[0].text\n",
    "    except:\n",
    "        title = np.nan\n",
    "    \n",
    "    text_container =  soup.find('meta', attrs={'name': 'description'})\n",
    "    \n",
    "    try:\n",
    "        all_text = text_container['content']\n",
    "    except:\n",
    "        all_text = np.nan\n",
    "    \n",
    "    try:\n",
    "        FB_shares_container = soup.find_all(\"strong\", {\"class\":\"js-share-count-text\"})\n",
    "        FB_shares = FB_shares_container[0].text.splitlines()\n",
    "        FB_shares = FB_shares[1].replace(\" \", \"\").replace(\"\\xa0\", \"\")\n",
    "    except:\n",
    "        FB_shares = np.nan\n",
    "        \n",
    "    try:\n",
    "        hearts_container = soup.find_all(\"div\", {\"class\":\"campaign-sp campaign-sp--heart fave-num\"})\n",
    "        hearts = hearts_container[0].text\n",
    "    except:\n",
    "        hearts = np.nan\n",
    "    \n",
    "    try:\n",
    "        content_dic = json.loads(re.sub(';$','',soup.find('script').text.split('= ')[1]))\n",
    "        location = content_dic['feed']['campaign']['location']['city']\n",
    "    except:\n",
    "        location = np.nan\n",
    "        \n",
    "    temp_row = np.array([[url, category, position, title, location, amount_raised, goal, NumDonators, timeFundraised, FB_shares, hearts, all_text]])\n",
    "    temp_df = pd.DataFrame(temp_row, columns = headers)\n",
    "    \n",
    "    return(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_all_urls(file = 'new.csv', start = 0, end = len(mydf)):\n",
    "    for i in range(start, end):\n",
    "        try:\n",
    "            temp_df = scrape_url(i)\n",
    "            temp_df.to_csv(file, mode = 'a',sep = '\\t', header = False)\n",
    "            print(\"Scraping url %s\" %(i+1))\n",
    "        except:\n",
    "            sleep(5)\n",
    "            temp_df = scrape_url(i)\n",
    "            temp_df.to_csv(file, mode = 'a',sep = '\\t', header = False)\n",
    "            print(\"Scraping url %s\" %(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping url 1\n",
      "Scraping url 2\n",
      "Scraping url 3\n",
      "Scraping url 4\n",
      "Scraping url 5\n",
      "Scraping url 6\n",
      "Scraping url 7\n",
      "Scraping url 8\n",
      "Scraping url 9\n",
      "Scraping url 10\n",
      "Scraping url 11\n",
      "Scraping url 12\n",
      "Scraping url 13\n",
      "Scraping url 14\n",
      "Scraping url 15\n",
      "Scraping url 16\n",
      "Scraping url 17\n",
      "Scraping url 18\n",
      "Scraping url 19\n",
      "Scraping url 20\n",
      "Scraping url 21\n",
      "Scraping url 22\n",
      "Scraping url 23\n",
      "Scraping url 24\n",
      "Scraping url 25\n",
      "Scraping url 26\n",
      "Scraping url 27\n",
      "Scraping url 28\n",
      "Scraping url 29\n",
      "Scraping url 30\n",
      "Scraping url 31\n",
      "Scraping url 32\n",
      "Scraping url 33\n",
      "Scraping url 34\n",
      "Scraping url 35\n",
      "Scraping url 36\n",
      "Scraping url 37\n",
      "Scraping url 38\n",
      "Scraping url 39\n",
      "Scraping url 40\n",
      "Scraping url 41\n",
      "Scraping url 42\n",
      "Scraping url 43\n",
      "Scraping url 44\n",
      "Scraping url 45\n",
      "Scraping url 46\n",
      "Scraping url 47\n",
      "Scraping url 48\n",
      "Scraping url 49\n",
      "Scraping url 50\n",
      "Scraping url 51\n",
      "Scraping url 52\n",
      "Scraping url 53\n",
      "Scraping url 54\n",
      "Scraping url 55\n",
      "Scraping url 56\n",
      "Scraping url 57\n",
      "Scraping url 58\n",
      "Scraping url 59\n",
      "Scraping url 60\n",
      "Scraping url 61\n",
      "Scraping url 62\n",
      "Scraping url 63\n",
      "Scraping url 64\n",
      "Scraping url 65\n",
      "Scraping url 66\n",
      "Scraping url 67\n",
      "Scraping url 68\n",
      "Scraping url 69\n",
      "Scraping url 70\n",
      "Scraping url 71\n",
      "Scraping url 72\n",
      "Scraping url 73\n",
      "Scraping url 74\n",
      "Scraping url 75\n",
      "Scraping url 76\n",
      "Scraping url 77\n",
      "Scraping url 78\n",
      "Scraping url 79\n",
      "Scraping url 80\n",
      "Scraping url 81\n",
      "Scraping url 82\n",
      "Scraping url 83\n",
      "Scraping url 84\n",
      "Scraping url 85\n",
      "Scraping url 86\n",
      "Scraping url 87\n",
      "Scraping url 88\n",
      "Scraping url 89\n",
      "Scraping url 90\n",
      "Scraping url 91\n",
      "Scraping url 92\n",
      "Scraping url 93\n",
      "Scraping url 94\n",
      "Scraping url 95\n",
      "Scraping url 96\n",
      "Scraping url 97\n",
      "Scraping url 98\n",
      "Scraping url 99\n",
      "Scraping url 100\n",
      "Scraping url 101\n",
      "Scraping url 102\n",
      "Scraping url 103\n",
      "Scraping url 104\n",
      "Scraping url 105\n",
      "Scraping url 106\n",
      "Scraping url 107\n",
      "Scraping url 108\n",
      "Scraping url 109\n",
      "Scraping url 110\n",
      "Scraping url 111\n",
      "Scraping url 112\n",
      "Scraping url 113\n",
      "Scraping url 114\n",
      "Scraping url 115\n",
      "Scraping url 116\n",
      "Scraping url 117\n",
      "Scraping url 118\n",
      "Scraping url 119\n",
      "Scraping url 120\n",
      "Scraping url 121\n",
      "Scraping url 122\n",
      "Scraping url 123\n",
      "Scraping url 124\n",
      "Scraping url 125\n",
      "Scraping url 126\n",
      "Scraping url 127\n",
      "Scraping url 128\n",
      "Scraping url 129\n",
      "Scraping url 130\n",
      "Scraping url 131\n",
      "Scraping url 132\n",
      "Scraping url 133\n",
      "Scraping url 134\n",
      "Scraping url 135\n",
      "Scraping url 136\n",
      "Scraping url 137\n",
      "Scraping url 138\n",
      "Scraping url 139\n",
      "Scraping url 140\n",
      "Scraping url 141\n",
      "Scraping url 142\n",
      "Scraping url 143\n",
      "Scraping url 144\n",
      "Scraping url 145\n"
     ]
    }
   ],
   "source": [
    "scrape_all_urls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
