{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Gauri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Gauri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Gauri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Gauri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Gauri\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from collections import defaultdict # Dictionaries with default values\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Url</th>\n",
       "      <th>Category</th>\n",
       "      <th>Position</th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Amount_Raised</th>\n",
       "      <th>Goal</th>\n",
       "      <th>Number_of_Donations</th>\n",
       "      <th>Length_of_Fundraising</th>\n",
       "      <th>FB_Shares</th>\n",
       "      <th>Number_of_Donors</th>\n",
       "      <th>Followers</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.gofundme.com/f/justiceforjacobblake</td>\n",
       "      <td>Medical</td>\n",
       "      <td>0</td>\n",
       "      <td>Justice for Jacob Blake</td>\n",
       "      <td>Kenosha, WI</td>\n",
       "      <td>2297930.0</td>\n",
       "      <td>3000000.0</td>\n",
       "      <td>73K</td>\n",
       "      <td>93 days 12:02:38.405126000</td>\n",
       "      <td>118K</td>\n",
       "      <td>72.5K</td>\n",
       "      <td>73.4K</td>\n",
       "      <td>On August 23rd my son was shot multiple times ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.gofundme.com/f/official-navajo-nat...</td>\n",
       "      <td>Medical</td>\n",
       "      <td>0</td>\n",
       "      <td>Official Navajo Nation COVID-19 Relief Fund</td>\n",
       "      <td>Window Rock, AZ</td>\n",
       "      <td>1862040.0</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>22.5K</td>\n",
       "      <td>205 days 12:02:39.366241000</td>\n",
       "      <td>71.7K</td>\n",
       "      <td>21.9K</td>\n",
       "      <td>22K</td>\n",
       "      <td>\\r\\nThe Navajo Nation COVID-19 Fund has been e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.gofundme.com/f/help-a-front-line-n...</td>\n",
       "      <td>Medical</td>\n",
       "      <td>0</td>\n",
       "      <td>Help a front line nurse and baby get proper care</td>\n",
       "      <td>Randolph, NJ</td>\n",
       "      <td>954793.0</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>19K</td>\n",
       "      <td>215 days 12:02:40.340314000</td>\n",
       "      <td>16.4K</td>\n",
       "      <td>18.3K</td>\n",
       "      <td>17.9K</td>\n",
       "      <td>On Sunday, April 12, Sylvia Leroy, a pregnant ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.gofundme.com/f/Tommy-Rivers-Rest-Up</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "      <td>Rest up, Tommy, we'll see you soon</td>\n",
       "      <td>Scottsdale, AZ</td>\n",
       "      <td>673179.0</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>11.3K</td>\n",
       "      <td>131 days 12:02:41.464483000</td>\n",
       "      <td>21.3K</td>\n",
       "      <td>10.3K</td>\n",
       "      <td>10.4K</td>\n",
       "      <td>First, thank you for being here. Tommy Rivers ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>https://www.gofundme.com/f/brandon039s-medical...</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "      <td>OFFICIAL BRANDON SAENZ MEDICAL FUND</td>\n",
       "      <td>Tyler, TX</td>\n",
       "      <td>570529.0</td>\n",
       "      <td>750000.0</td>\n",
       "      <td>24.7K</td>\n",
       "      <td>175 days 12:02:42.383091000</td>\n",
       "      <td>5.5K</td>\n",
       "      <td>24.3K</td>\n",
       "      <td>24.5K</td>\n",
       "      <td>My name is Melissa Green and I am the mother o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                                Url Category  \\\n",
       "0           0    https://www.gofundme.com/f/justiceforjacobblake  Medical   \n",
       "1           0  https://www.gofundme.com/f/official-navajo-nat...  Medical   \n",
       "2           0  https://www.gofundme.com/f/help-a-front-line-n...  Medical   \n",
       "3           0    https://www.gofundme.com/f/Tommy-Rivers-Rest-Up  Medical   \n",
       "4           0  https://www.gofundme.com/f/brandon039s-medical...  Medical   \n",
       "\n",
       "   Position                                             Title  \\\n",
       "0         0                           Justice for Jacob Blake   \n",
       "1         0       Official Navajo Nation COVID-19 Relief Fund   \n",
       "2         0  Help a front line nurse and baby get proper care   \n",
       "3         1                Rest up, Tommy, we'll see you soon   \n",
       "4         1               OFFICIAL BRANDON SAENZ MEDICAL FUND   \n",
       "\n",
       "          Location  Amount_Raised       Goal Number_of_Donations  \\\n",
       "0      Kenosha, WI      2297930.0  3000000.0                 73K   \n",
       "1  Window Rock, AZ      1862040.0  1000000.0               22.5K   \n",
       "2     Randolph, NJ       954793.0  1200000.0                 19K   \n",
       "3   Scottsdale, AZ       673179.0  1000000.0               11.3K   \n",
       "4        Tyler, TX       570529.0   750000.0               24.7K   \n",
       "\n",
       "         Length_of_Fundraising FB_Shares Number_of_Donors Followers  \\\n",
       "0   93 days 12:02:38.405126000      118K            72.5K     73.4K   \n",
       "1  205 days 12:02:39.366241000     71.7K            21.9K       22K   \n",
       "2  215 days 12:02:40.340314000     16.4K            18.3K     17.9K   \n",
       "3  131 days 12:02:41.464483000     21.3K            10.3K     10.4K   \n",
       "4  175 days 12:02:42.383091000      5.5K            24.3K     24.5K   \n",
       "\n",
       "                                                Text  \n",
       "0  On August 23rd my son was shot multiple times ...  \n",
       "1  \\r\\nThe Navajo Nation COVID-19 Fund has been e...  \n",
       "2  On Sunday, April 12, Sylvia Leroy, a pregnant ...  \n",
       "3  First, thank you for being here. Tommy Rivers ...  \n",
       "4  My name is Melissa Green and I am the mother o...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## importing data sets and dropping nan's\n",
    "df = pd.read_csv('GFM_data.csv',sep = '\\t')\n",
    "df = df.loc[df['Text'].dropna().index]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text preprocessing using Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "REPLACE_IP_ADDRESS = re.compile(r'\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b')\n",
    "\n",
    "def clean_text(x):\n",
    "    ## normalizing text by stripping white space and lower casing\n",
    "    x = extract_entities(x)\n",
    "    x =  x.lower().strip()\n",
    "    ## removing urls\n",
    "    x = re.sub(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))', '', x)\n",
    "    ## removing phone numbers\n",
    "    x = re.sub('\\([0-9]{3}\\)\\s*[0-9]{3}-[0-9]{4}','',x)\n",
    "    ## strip all non alphanumeric things\n",
    "    x = re.sub('\\n',' ',x)\n",
    "    x = re.sub(\"[^a-zA-Z0-9 #]\",'',x)\n",
    "    x = re.sub(\"\\s+\",' ',x)\n",
    "    text = x.replace('\\n', ' ').lower()# lowercase text\n",
    "    text = REPLACE_IP_ADDRESS.sub('', text) # remove ip address\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ',text)# replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('',text)# delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join([w for w in text.split() if not w in STOPWORDS])# delete stopwords from text\n",
    "    \n",
    "    return text\n",
    "\n",
    "def extract_entities(text):\n",
    "    names = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label'):\n",
    "                names.append(' '.join(c[0] for c in chunk.leaves()))\n",
    "    new_text = text\n",
    "    for name in names:\n",
    "        if name in text:\n",
    "            new_text = new_text.replace(name, '')\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Text'] = df['Text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'covid19 fund established help respond covid19 pandemic official covid19 fundraising donation effortthe accepting monetary nonmonetary donations address immediate medical community needs coordinating donations office nonmonetary donations also accepted information see call official information covid19 please see official government website'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = df['Text'][1]\n",
    "#print(extract_entities(text))\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {i: idx for idx,i in enumerate(df['Category'].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df['Text']\n",
    "y = [categories[i] for i in df['Category']]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BoW (Bag of Words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse as sp_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of all words from train corpus with their counts.\n",
    "words_counts = {}\n",
    "for comments in X_train:\n",
    "    for word in comments.split():\n",
    "        if word not in words_counts:\n",
    "            words_counts[word] = 0\n",
    "        words_counts[word] += 1\n",
    "##most pop words        \n",
    "DICT_SIZE = 10000\n",
    "POPULAR_WORDS = sorted(words_counts, key=words_counts.get, reverse=True)[:DICT_SIZE]\n",
    "## same dics but flopped\n",
    "WORDS_TO_INDEX = {key: rank for rank, key in enumerate(POPULAR_WORDS, 0)}\n",
    "INDEX_TO_WORDS = {index:word for word, index in WORDS_TO_INDEX.items()}\n",
    "ALL_WORDS = WORDS_TO_INDEX.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape  (646, 10000) \n",
      "X_val shape  (216, 10000)\n"
     ]
    }
   ],
   "source": [
    "def my_bag_of_words(text, words_to_index, dict_size):\n",
    "    result_vector = np.zeros(dict_size)\n",
    "    for word in text.split(' '):\n",
    "        if word in words_to_index:\n",
    "            result_vector[words_to_index[word]] +=1\n",
    "    return result_vector\n",
    "\n",
    "X_train_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_train])\n",
    "X_test_mybag = sp_sparse.vstack([sp_sparse.csr_matrix(my_bag_of_words(text, WORDS_TO_INDEX, DICT_SIZE)) for text in X_test])\n",
    "print('X_train shape ', X_train_mybag.shape, '\\nX_val shape ', X_test_mybag.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1.0\n",
      "  (0, 1)\t5.0\n",
      "  (0, 2)\t5.0\n",
      "  (0, 3)\t4.0\n",
      "  (0, 4)\t2.0\n",
      "  (0, 5)\t2.0\n",
      "  (0, 6)\t3.0\n",
      "  (0, 7)\t5.0\n",
      "  (0, 8)\t1.0\n",
      "  (0, 9)\t2.0\n",
      "  (0, 11)\t3.0\n",
      "  (0, 12)\t1.0\n",
      "  (0, 15)\t3.0\n",
      "  (0, 16)\t1.0\n",
      "  (0, 17)\t1.0\n",
      "  (0, 18)\t1.0\n",
      "  (0, 19)\t2.0\n",
      "  (0, 20)\t1.0\n",
      "  (0, 23)\t2.0\n",
      "  (0, 25)\t1.0\n",
      "  (0, 31)\t1.0\n",
      "  (0, 35)\t1.0\n",
      "  (0, 39)\t1.0\n",
      "  (0, 41)\t4.0\n",
      "  (0, 53)\t2.0\n",
      "  :\t:\n",
      "  (0, 6786)\t1.0\n",
      "  (0, 6787)\t1.0\n",
      "  (0, 6788)\t1.0\n",
      "  (0, 6789)\t1.0\n",
      "  (0, 6790)\t1.0\n",
      "  (0, 6791)\t1.0\n",
      "  (0, 6792)\t1.0\n",
      "  (0, 6793)\t1.0\n",
      "  (0, 6794)\t1.0\n",
      "  (0, 6795)\t1.0\n",
      "  (0, 6796)\t1.0\n",
      "  (0, 6797)\t1.0\n",
      "  (0, 6798)\t1.0\n",
      "  (0, 6799)\t1.0\n",
      "  (0, 6800)\t1.0\n",
      "  (0, 6801)\t1.0\n",
      "  (0, 6802)\t1.0\n",
      "  (0, 6803)\t1.0\n",
      "  (0, 6804)\t1.0\n",
      "  (0, 6805)\t1.0\n",
      "  (0, 6806)\t1.0\n",
      "  (0, 6807)\t1.0\n",
      "  (0, 6808)\t1.0\n",
      "  (0, 6809)\t1.0\n",
      "  (0, 6810)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print(X_train_mybag[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['help',\n",
       " 'family',\n",
       " 'us',\n",
       " 'support',\n",
       " 'community',\n",
       " 'time',\n",
       " 'many',\n",
       " 'years',\n",
       " 'life',\n",
       " 'would']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "POPULAR_WORDS[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating tfidf vector\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), max_df=0.9, min_df=5)\n",
    "## transforming it\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3297)\t0.05815891535674012\n",
      "  (0, 1341)\t0.045302859490367046\n",
      "  (0, 3156)\t0.06194505162305915\n",
      "  (0, 2053)\t0.04781574533158471\n",
      "  (0, 2050)\t0.06934429806019397\n",
      "  (0, 3637)\t0.040835811309454344\n",
      "  (0, 3395)\t0.06746254340756448\n",
      "  (0, 3642)\t0.06583249338362124\n",
      "  (0, 1307)\t0.06583249338362124\n",
      "  (0, 3023)\t0.06746254340756448\n",
      "  (0, 1915)\t0.06934429806019397\n",
      "  (0, 91)\t0.06194505162305915\n",
      "  (0, 3613)\t0.05464711068016739\n",
      "  (0, 1180)\t0.13492508681512896\n",
      "  (0, 1196)\t0.06746254340756448\n",
      "  (0, 1836)\t0.052421467315142106\n",
      "  (0, 1536)\t0.04853402555458001\n",
      "  (0, 1786)\t0.042750425314352254\n",
      "  (0, 3106)\t0.06746254340756448\n",
      "  (0, 1339)\t0.02716504688906592\n",
      "  (0, 1401)\t0.04207829728998669\n",
      "  (0, 169)\t0.04186223426909249\n",
      "  (0, 1478)\t0.04890966263856938\n",
      "  (0, 853)\t0.05192314334923948\n",
      "  (0, 2292)\t0.04713739071080929\n",
      "  :\t:\n",
      "  (0, 3578)\t0.06746254340756448\n",
      "  (0, 3147)\t0.07981660488118153\n",
      "  (0, 1357)\t0.05011134442958777\n",
      "  (0, 3292)\t0.04392221890501285\n",
      "  (0, 1097)\t0.07606668108801881\n",
      "  (0, 2703)\t0.06439468736424077\n",
      "  (0, 3657)\t0.12005912584605698\n",
      "  (0, 2405)\t0.07941637567312605\n",
      "  (0, 467)\t0.043219992513135755\n",
      "  (0, 1522)\t0.035372135520985566\n",
      "  (0, 3612)\t0.04103430742273091\n",
      "  (0, 112)\t0.06934429806019397\n",
      "  (0, 2379)\t0.2844834388942435\n",
      "  (0, 1112)\t0.039708187836563026\n",
      "  (0, 477)\t0.14000125041675046\n",
      "  (0, 1610)\t0.06042911993151484\n",
      "  (0, 1987)\t0.10500093781256285\n",
      "  (0, 1177)\t0.10458215955710133\n",
      "  (0, 3412)\t0.06934429806019397\n",
      "  (0, 70)\t0.06439468736424077\n",
      "  (0, 2898)\t0.04474788928826099\n",
      "  (0, 2153)\t0.05815891535674012\n",
      "  (0, 90)\t0.10696727250106643\n",
      "  (0, 2905)\t0.05405151733908535\n",
      "  (0, 1381)\t0.13868859612038795\n"
     ]
    }
   ],
   "source": [
    "print(X_train_tfidf[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "def train_classifier(X_train, y_train, C, regularisation):\n",
    "    model = OneVsRestClassifier(LogisticRegression(penalty=regularisation, C=C, max_iter=10000)).fit(X_train, y_train)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gauri\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "classifier_mybag = train_classifier(X_train_mybag, y_train, C = 4, regularisation = 'l2')\n",
    "classifier_tfidf = train_classifier(X_train_tfidf, y_train, C = 4, regularisation = 'l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicted_labels_mybag = classifier_mybag.predict(X_test_mybag)\n",
    "y_test_predicted_labels_tfidf = classifier_tfidf.predict(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_predicted_scores_mybag = classifier_mybag.decision_function(X_test_mybag)\n",
    "y_test_predicted_scores_tfidf = classifier_tfidf.decision_function(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ -3.9297238 ,  -5.44539157,  -5.38842603, ...,   1.35784488,\n",
       "         -5.11161354,  -3.26642373],\n",
       "       [ -6.11214001,  -3.52481254,  -3.12973523, ...,  -9.92758128,\n",
       "         -8.10737305,  -8.06886824],\n",
       "       [ -5.23018853,  -5.54876435,  -5.48575431, ...,  -7.99839621,\n",
       "         -5.77359911,  -4.23833348],\n",
       "       ...,\n",
       "       [ -4.6246088 ,  -5.29754432,  -5.11185891, ...,  -2.94640428,\n",
       "         -3.41343855,  -3.33371116],\n",
       "       [ -5.78135692,  -5.13609206,  -0.8193948 , ..., -13.51251625,\n",
       "         -6.73438193,  -9.6076361 ],\n",
       "       [-12.32348296, -14.71793571, -10.3470708 , ...,  -7.63145754,\n",
       "         -5.88014374,  -9.59807245]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_predicted_scores_mybag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-words\n",
      "\n",
      "Accuracy:  100\n",
      "F1-score weighted:  0.45880775829101395\n",
      "\n",
      "Tfidf\n",
      "\n",
      "Accuracy:  116\n",
      "F1-score weighted:  0.5301577679706805\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gauri\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "classifier_tfidf = train_classifier(X_train_tfidf, y_train, C = 50, regularisation = 'l2')\n",
    "\n",
    "y_test_predicted_labels_mybag = classifier_mybag.predict(X_test_mybag)\n",
    "y_test_predicted_labels_tfidf = classifier_tfidf.predict(X_test_tfidf)\n",
    "\n",
    "def print_evaluation_scores(y_test, predicted):\n",
    "    \n",
    "    print('Accuracy: ', accuracy_score(y_test, predicted, normalize=False))\n",
    "    print('F1-score weighted: ', f1_score(y_test, predicted, average='weighted'))\n",
    "    \n",
    "print('Bag-of-words\\n')\n",
    "print_evaluation_scores(y_test, y_test_predicted_labels_mybag)\n",
    "print('\\nTfidf\\n')\n",
    "print_evaluation_scores(y_test, y_test_predicted_labels_tfidf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
