{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df = pd.read_csv('GFM_data.csv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf # build model\n",
    "import string # get set of punctuations\n",
    "import requests # get data file in notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('averaged_perceptron_tagger')\n",
    "#nltk.download('maxent_ne_chunker')\n",
    "#nltk.download('words')\n",
    "#nltk.download('wordnet')\n",
    "#nltk.download('stopwords')\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "REPLACE_IP_ADDRESS = re.compile(r'\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b')\n",
    "def extract_entities(text):\n",
    "    names = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label'):\n",
    "                names.append(' '.join(c[0] for c in chunk.leaves()))\n",
    "    new_text = text\n",
    "    for name in names:\n",
    "        if name in text:\n",
    "            new_text = new_text.replace(name, 'NLP')\n",
    "    return new_text\n",
    "\n",
    "def clean_text(x):\n",
    "    ## removing names\n",
    "    x = extract_entities(x)\n",
    "    ## normalizing text by stripping white space and lower casing\n",
    "    x =  x.lower().strip()\n",
    "    ## removing urls\n",
    "    x = re.sub(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))', '', x)\n",
    "    ## removing phone numbers\n",
    "    x = re.sub('\\([0-9]{3}\\)\\s*[0-9]{3}-[0-9]{4}','',x)\n",
    "    ## strip all non alphanumeric things\n",
    "    x = re.sub('\\n',' ',x)\n",
    "    x = re.sub(\"[^a-zA-Z0-9 #]\",'',x)\n",
    "    x = re.sub(\"\\s+\",' ',x)\n",
    "    text = x.replace('\\n', ' ').lower()# lowercase text\n",
    "    text = REPLACE_IP_ADDRESS.sub('', text) # remove ip address\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ',text)# replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('',text)# delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    # originally:\n",
    "    # text = ' '.join([w for w in text.split() if not w in STOPWORDS])# delete stopwords from text\n",
    "    text = [w for w in text.split() if not w in STOPWORDS]# delete stopwords from text   \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_nan = df.dropna()\n",
    "no_nan['Text'] = no_nan['Text'].apply(clean_text)\n",
    "text = no_nan.get('Text').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text # check what's inside"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [] # combine words\n",
    "for lst in text:\n",
    "    tokens += lst\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = ' '.join(text) # join all array element\n",
    "#new_text = clean_text(text)\n",
    "#new_text\n",
    "length = 51 # learn initial 50 words, predict the next word\n",
    "lines = [] # lst of all training sequences\n",
    "tokens = text[0]\n",
    "sufficiency = 200_000\n",
    "for i in range(length, len(tokens)):\n",
    "    seq = tokens[i-length:i]\n",
    "    line = ' '.join(seq)\n",
    "    lines.append(line)\n",
    "    if i > sufficiency:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import LambdaCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(oov_token=\"<OOV>\") # OOV adds new words outside the trained words\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines) # all words become numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:, :-1], sequences[:,-1] # 50 first words in X, the last word in y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = len(X[0]) # Length of each list in X is 50 (might also use X.shape[1])\n",
    " \n",
    "# LSTM model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length = sequence_length)) # input dim - vocabsize, output dim = 50\n",
    "# 100 hidden layers\n",
    "model.add(LSTM(100, return_sequences=True)) # do twice, return_sequences=True\n",
    "model.add(LSTM(100)) \n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_seq(model, tokenizer, text_seq_length, seed_text, n_words):\n",
    "    text = []\n",
    "    \n",
    "    for _ in range(n_words):\n",
    "        encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        encoded = pad_sequences([encoded], maxlen = text_seq_length, truncating='pre')\n",
    "\n",
    "        y_predict = model.predict_classes(encoded)\n",
    "\n",
    "        predicted_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == y_predict:\n",
    "                predicted_word = word\n",
    "                break\n",
    "        seed_text = seed_text + ' ' + predicted_word\n",
    "        text.append(predicted_word)\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alltext = df['Text'].str.cat(sep='. ').lower()\n",
    "#alltext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y, batch_size=256, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_text=alltext[:len(df['Text'][0])+16]\n",
    "seq_length = len(X[0])\n",
    "#seed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_seq(model, tokenizer, text_seq_length, seed_text, n_words):\n",
    "    text = []\n",
    "\n",
    "    for _ in range(n_words):\n",
    "        encoded = tokenizer.texts_to_sequences([seed_text])[0]\n",
    "        encoded = pad_sequences([encoded], maxlen = text_seq_length, truncating='pre')\n",
    "\n",
    "        y_predict = model.predict_classes(encoded)\n",
    "\n",
    "        predicted_word = ''\n",
    "        for word, index in tokenizer.word_index.items():\n",
    "            if index == y_predict:\n",
    "                predicted_word = word\n",
    "                break\n",
    "        print('--SEED TEXT--')\n",
    "        print(seed_text)\n",
    "        seed_text = seed_text + ' ' + predicted_word\n",
    "        print('--PREDICTED WORDS--', predicted_word)\n",
    "        print('----------------------------------------------------------')\n",
    "        text.append(predicted_word)\n",
    "    return ' '.join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text_seq(model, tokenizer, seq_length, seed_text, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://kgptalkie.com/text-generation-using-tensorflow-keras-and-lstm/\n",
    "\n",
    "https://www.youtube.com/watch?v=VAMKuRAh2nc&t=1607s&ab_channel=KGPTalkie"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
