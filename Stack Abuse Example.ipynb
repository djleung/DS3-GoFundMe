{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sddjl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sddjl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\sddjl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\sddjl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sddjl\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from collections import defaultdict # Dictionaries with default values\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import numpy as np\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Url</th>\n",
       "      <th>Category</th>\n",
       "      <th>Position</th>\n",
       "      <th>Title</th>\n",
       "      <th>Location</th>\n",
       "      <th>Amount_Raised</th>\n",
       "      <th>Goal</th>\n",
       "      <th>Number_of_Donations</th>\n",
       "      <th>Length_of_Fundraising</th>\n",
       "      <th>FB_Shares</th>\n",
       "      <th>Number_of_Donors</th>\n",
       "      <th>Followers</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.gofundme.com/f/justiceforjacobblake</td>\n",
       "      <td>Medical</td>\n",
       "      <td>0</td>\n",
       "      <td>Justice for Jacob Blake</td>\n",
       "      <td>Kenosha, WI</td>\n",
       "      <td>2297930.0</td>\n",
       "      <td>3000000.0</td>\n",
       "      <td>73K</td>\n",
       "      <td>93 days 12:02:38.405126000</td>\n",
       "      <td>118K</td>\n",
       "      <td>72.5K</td>\n",
       "      <td>73.4K</td>\n",
       "      <td>On August 23rd my son was shot multiple times ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.gofundme.com/f/official-navajo-nat...</td>\n",
       "      <td>Medical</td>\n",
       "      <td>0</td>\n",
       "      <td>Official Navajo Nation COVID-19 Relief Fund</td>\n",
       "      <td>Window Rock, AZ</td>\n",
       "      <td>1862040.0</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>22.5K</td>\n",
       "      <td>205 days 12:02:39.366241000</td>\n",
       "      <td>71.7K</td>\n",
       "      <td>21.9K</td>\n",
       "      <td>22K</td>\n",
       "      <td>\\r\\nThe Navajo Nation COVID-19 Fund has been e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.gofundme.com/f/help-a-front-line-n...</td>\n",
       "      <td>Medical</td>\n",
       "      <td>0</td>\n",
       "      <td>Help a front line nurse and baby get proper care</td>\n",
       "      <td>Randolph, NJ</td>\n",
       "      <td>954793.0</td>\n",
       "      <td>1200000.0</td>\n",
       "      <td>19K</td>\n",
       "      <td>215 days 12:02:40.340314000</td>\n",
       "      <td>16.4K</td>\n",
       "      <td>18.3K</td>\n",
       "      <td>17.9K</td>\n",
       "      <td>On Sunday, April 12, Sylvia Leroy, a pregnant ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.gofundme.com/f/Tommy-Rivers-Rest-Up</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "      <td>Rest up, Tommy, we'll see you soon</td>\n",
       "      <td>Scottsdale, AZ</td>\n",
       "      <td>673179.0</td>\n",
       "      <td>1000000.0</td>\n",
       "      <td>11.3K</td>\n",
       "      <td>131 days 12:02:41.464483000</td>\n",
       "      <td>21.3K</td>\n",
       "      <td>10.3K</td>\n",
       "      <td>10.4K</td>\n",
       "      <td>First, thank you for being here. Tommy Rivers ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>https://www.gofundme.com/f/brandon039s-medical...</td>\n",
       "      <td>Medical</td>\n",
       "      <td>1</td>\n",
       "      <td>OFFICIAL BRANDON SAENZ MEDICAL FUND</td>\n",
       "      <td>Tyler, TX</td>\n",
       "      <td>570529.0</td>\n",
       "      <td>750000.0</td>\n",
       "      <td>24.7K</td>\n",
       "      <td>175 days 12:02:42.383091000</td>\n",
       "      <td>5.5K</td>\n",
       "      <td>24.3K</td>\n",
       "      <td>24.5K</td>\n",
       "      <td>My name is Melissa Green and I am the mother o...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                                Url Category  \\\n",
       "0           0    https://www.gofundme.com/f/justiceforjacobblake  Medical   \n",
       "1           0  https://www.gofundme.com/f/official-navajo-nat...  Medical   \n",
       "2           0  https://www.gofundme.com/f/help-a-front-line-n...  Medical   \n",
       "3           0    https://www.gofundme.com/f/Tommy-Rivers-Rest-Up  Medical   \n",
       "4           0  https://www.gofundme.com/f/brandon039s-medical...  Medical   \n",
       "\n",
       "   Position                                             Title  \\\n",
       "0         0                           Justice for Jacob Blake   \n",
       "1         0       Official Navajo Nation COVID-19 Relief Fund   \n",
       "2         0  Help a front line nurse and baby get proper care   \n",
       "3         1                Rest up, Tommy, we'll see you soon   \n",
       "4         1               OFFICIAL BRANDON SAENZ MEDICAL FUND   \n",
       "\n",
       "          Location  Amount_Raised       Goal Number_of_Donations  \\\n",
       "0      Kenosha, WI      2297930.0  3000000.0                 73K   \n",
       "1  Window Rock, AZ      1862040.0  1000000.0               22.5K   \n",
       "2     Randolph, NJ       954793.0  1200000.0                 19K   \n",
       "3   Scottsdale, AZ       673179.0  1000000.0               11.3K   \n",
       "4        Tyler, TX       570529.0   750000.0               24.7K   \n",
       "\n",
       "         Length_of_Fundraising FB_Shares Number_of_Donors Followers  \\\n",
       "0   93 days 12:02:38.405126000      118K            72.5K     73.4K   \n",
       "1  205 days 12:02:39.366241000     71.7K            21.9K       22K   \n",
       "2  215 days 12:02:40.340314000     16.4K            18.3K     17.9K   \n",
       "3  131 days 12:02:41.464483000     21.3K            10.3K     10.4K   \n",
       "4  175 days 12:02:42.383091000      5.5K            24.3K     24.5K   \n",
       "\n",
       "                                                Text  \n",
       "0  On August 23rd my son was shot multiple times ...  \n",
       "1  \\r\\nThe Navajo Nation COVID-19 Fund has been e...  \n",
       "2  On Sunday, April 12, Sylvia Leroy, a pregnant ...  \n",
       "3  First, thank you for being here. Tommy Rivers ...  \n",
       "4  My name is Melissa Green and I am the mother o...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## importing data sets and dropping nan's\n",
    "df = pd.read_csv('GFM_data.csv',sep = '\\t')\n",
    "df = df.loc[df['Text'].dropna().index]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    8.370000e+02\n",
       "mean     1.157495e+05\n",
       "std      3.218705e+05\n",
       "min      6.370000e+02\n",
       "25%      1.954500e+04\n",
       "50%      5.305800e+04\n",
       "75%      1.233970e+05\n",
       "max      6.750030e+06\n",
       "Name: Amount_Raised, dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Amount_Raised'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['Amount_Raised'] > 123397] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(stopwords.words('english'))\n",
    "REPLACE_IP_ADDRESS = re.compile(r'\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}\\b')\n",
    "\n",
    "def extract_entities(text):\n",
    "    names = []\n",
    "    for sent in nltk.sent_tokenize(text):\n",
    "        for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "            if hasattr(chunk, 'label'):\n",
    "                names.append(' '.join(c[0] for c in chunk.leaves()))\n",
    "    new_text = text\n",
    "    for name in names:\n",
    "        if name in text:\n",
    "            new_text = new_text.replace(name, 'NLP')\n",
    "    return new_text\n",
    "\n",
    "def clean_text(x):\n",
    "    ## removing names\n",
    "    x = extract_entities(x)\n",
    "    ## normalizing text by stripping white space and lower casing\n",
    "    x =  x.lower().strip()\n",
    "    ## removing urls\n",
    "    x = re.sub(r'(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:\\'\".,<>?«»“”‘’]))', '', x)\n",
    "    ## removing phone numbers\n",
    "    x = re.sub('\\([0-9]{3}\\)\\s*[0-9]{3}-[0-9]{4}','',x)\n",
    "    ## strip all non alphanumeric things\n",
    "    x = re.sub('\\n',' ',x)\n",
    "    x = re.sub(\"[^a-zA-Z0-9 #]\",'',x)\n",
    "    x = re.sub(\"\\s+\",' ',x)\n",
    "    text = x.replace('\\n', ' ').lower()# lowercase text\n",
    "    text = REPLACE_IP_ADDRESS.sub('', text) # remove ip address\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ',text)# replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    text = BAD_SYMBOLS_RE.sub('',text)# delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "    text = ' '.join([w for w in text.split() if not w in STOPWORDS])# delete stopwords from text\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['Text'][4]\n",
    "new_text = clean_text(text)\n",
    "df['Text'] = df['Text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'august 23rd son shot multiple times back nlp police department officer son broke altercation unrelated party shooting left son critically injured fights life extent sons injuries unknown remain prayerful continues undergo multiple rounds operations save life fund established cover sons medical expenses mental grief counseling family assist family days come continue seek justice nlp portion proceeds also used benefit sons six children witnessed horrific act violence anyone wishing send cards letters encouragement andor contributions form money order check may mail nlp co nlp nlp 122 calhoun street tallahassee fl 32301 attn nlp'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = ''\n",
    "for text in df['Text']:\n",
    "    file += text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Stack Abuse Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import sys\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(input):\n",
    "    # lowercase everything to standardize it\n",
    "    input = input.lower()\n",
    "\n",
    "    # instantiate the tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(input)\n",
    "\n",
    "    # if the created token isn't in the stop words, make it part of \"filtered\"\n",
    "    filtered = filter(lambda token: token not in stopwords.words('english'), tokens)\n",
    "    return \" \".join(filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the input data, make tokens\n",
    "processed_inputs = tokenize_words(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(processed_inputs)))\n",
    "char_to_num = dict((c, i) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters: 268097\n",
      "Total vocab: 37\n"
     ]
    }
   ],
   "source": [
    "input_len = len(processed_inputs)\n",
    "vocab_len = len(chars)\n",
    "print (\"Total number of characters:\", input_len)\n",
    "print (\"Total vocab:\", vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100\n",
    "x_data = []\n",
    "y_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through inputs, start at the beginning and go until we hit\n",
    "# the final character we can create a sequence out of\n",
    "for i in range(0, input_len - seq_length, 1):\n",
    "    # Define input and output sequences\n",
    "    # Input is the current character plus desired sequence length\n",
    "    in_seq = processed_inputs[i:i + seq_length]\n",
    "\n",
    "    # Out sequence is the initial character plus total sequence length\n",
    "    out_seq = processed_inputs[i + seq_length]\n",
    "\n",
    "    # We now convert list of characters to integers based on\n",
    "    # previously and add the values to our lists\n",
    "    x_data.append([char_to_num[char] for char in in_seq])\n",
    "    y_data.append(char_to_num[out_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns: 267997\n"
     ]
    }
   ],
   "source": [
    "n_patterns = len(x_data)\n",
    "print (\"Total Patterns:\", n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = numpy.reshape(x_data, (n_patterns, seq_length, 1))\n",
    "X = X/float(vocab_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np_utils.to_categorical(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(256, input_shape=(X.shape[1], X.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y.shape[1], activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"model_weights_saved.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "desired_callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "267997/267997 [==============================] - 4269s 16ms/step - loss: 2.8135\n",
      "\n",
      "Epoch 00001: loss improved from inf to 2.81351, saving model to model_weights_saved.hdf5\n",
      "Epoch 2/10\n",
      "267997/267997 [==============================] - 4260s 16ms/step - loss: 2.4211\n",
      "\n",
      "Epoch 00002: loss improved from 2.81351 to 2.42113, saving model to model_weights_saved.hdf5\n",
      "Epoch 3/10\n",
      "267997/267997 [==============================] - 4286s 16ms/step - loss: 2.2587\n",
      "\n",
      "Epoch 00003: loss improved from 2.42113 to 2.25868, saving model to model_weights_saved.hdf5\n",
      "Epoch 4/10\n",
      "267997/267997 [==============================] - 4291s 16ms/step - loss: 2.1524\n",
      "\n",
      "Epoch 00004: loss improved from 2.25868 to 2.15237, saving model to model_weights_saved.hdf5\n",
      "Epoch 5/10\n",
      "267997/267997 [==============================] - 28312s 106ms/step - loss: 2.0751\n",
      "\n",
      "Epoch 00005: loss improved from 2.15237 to 2.07512, saving model to model_weights_saved.hdf5\n",
      "Epoch 6/10\n",
      "267997/267997 [==============================] - 4288s 16ms/step - loss: 2.0153\n",
      "\n",
      "Epoch 00006: loss improved from 2.07512 to 2.01532, saving model to model_weights_saved.hdf5\n",
      "Epoch 7/10\n",
      "267997/267997 [==============================] - 4266s 16ms/step - loss: 1.9694\n",
      "\n",
      "Epoch 00007: loss improved from 2.01532 to 1.96941, saving model to model_weights_saved.hdf5\n",
      "Epoch 8/10\n",
      "267997/267997 [==============================] - 4269s 16ms/step - loss: 1.9293\n",
      "\n",
      "Epoch 00008: loss improved from 1.96941 to 1.92933, saving model to model_weights_saved.hdf5\n",
      "Epoch 9/10\n",
      "267997/267997 [==============================] - 4258s 16ms/step - loss: 1.8976\n",
      "\n",
      "Epoch 00009: loss improved from 1.92933 to 1.89759, saving model to model_weights_saved.hdf5\n",
      "Epoch 10/10\n",
      "267997/267997 [==============================] - 4258s 16ms/step - loss: 1.8700\n",
      "\n",
      "Epoch 00010: loss improved from 1.89759 to 1.87004, saving model to model_weights_saved.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x19a2ced0a88>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, epochs=10, batch_size=128, callbacks=desired_callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"model_weights_saved.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_to_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Seed:\n",
      "\" sinesses phase 3 movie theaters allowed operate 25 capacity 3 screens even sell would equal 69 peopl \"\n"
     ]
    }
   ],
   "source": [
    "start = numpy.random.randint(0, len(x_data) - 1)\n",
    "pattern = x_data[start]\n",
    "print(\"Random Seed:\")\n",
    "print(\"\\\"\", ''.join([num_to_char[value] for value in pattern]), \"\\\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nlp nl"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(vocab_len)\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = num_to_char[index]\n",
    "    seq_in = [num_to_char[value] for value in pattern]\n",
    "\n",
    "    sys.stdout.write(result)\n",
    "\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
